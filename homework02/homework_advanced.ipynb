{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NqtXrZApBkM4"
   },
   "source": [
    "# Optimizing training and inference\n",
    "\n",
    "In this notebook, we will discuss different ways to reduce memory and compute usage during training and inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NEt8wg4JCQdm"
   },
   "source": [
    "## Prepare training script (1 point)\n",
    "\n",
    "When training large models, it is usually a best practice not to use Jupyter notebooks, but run a **separate script** for training which could have command-line flags for various hyperparameters and training modes. This is especially useful when you need to run multiple experiments simultaneously (e.g. on a cluster with task scheduler). Another advantage of this is that after training, the process will finish and free the resources for other users of a shared GPU.\n",
    "\n",
    "In this part, you will need to put all your code to train a model on Tiny ImageNet that you wrote for the previous task in `train.py`.\n",
    "\n",
    "You can then run your script from inside of this notebook like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6-TWiKq8H9yT"
   },
   "outputs": [],
   "source": [
    "!python3 train_baby_resnet.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See `v_num=10` in Tensorboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task** \n",
    "\n",
    "Write code for training with architecture from homework_part2\n",
    "\n",
    "**Requirements**\n",
    "* Optional arguments from command line such as batch size and number of epochs with built-in argparse\n",
    "* Modular structure - separate functions for creating data generator, building model and training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tKPYZ3QLEqX8"
   },
   "source": [
    "## Profiling time (1 point)\n",
    "\n",
    "For the next tasks, you need to add measurements to your training loop. You can use [`perf_counter`](https://docs.python.org/3/library/time.html#time.perf_counter) for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bSr-PyQNFkSC"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HMJMCGRKFYCc",
    "outputId": "571046a2-443b-465f-ce62-ddaf68b105d0"
   },
   "outputs": [],
   "source": [
    "x = np.random.randn(1000, 1000)\n",
    "y = np.random.randn(1000, 1000)\n",
    "\n",
    "start_counter = time.perf_counter()\n",
    "z = x @ y\n",
    "elapsed_time = time.perf_counter() - start_counter\n",
    "print(f\"Matrix multiplication took {elapsed_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FfhLeWjTGTpB"
   },
   "source": [
    "**Task**. You need to add the following measurements to your training script:\n",
    "* How much time a forward-backward pass takes for a single batch;\n",
    "* How much time an epoch takes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***As you can see, PL (Tensorboard) does this automatically***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "khDOTn_SHaND"
   },
   "source": [
    "## Profiling memory usage (1 point)\n",
    "\n",
    "**Task**. You need to measure the memory consumptions\n",
    "\n",
    "This section depends on whether you train on CPU or GPU.\n",
    "\n",
    "### If you train on CPU\n",
    "You can use GNU time to measure peak RAM usage of a script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "98xvXSjUIDzl"
   },
   "outputs": [],
   "source": [
    "!/usr/bin/time -lp python train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v1ES2Pc9IlH5"
   },
   "source": [
    "**Maximum resident set size**  will show you the peak RAM usage in bytes after the script finishes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**. \n",
    "Imports also require memory, do the correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kq5lY5CKJHX1"
   },
   "source": [
    "### If you train on GPU\n",
    "\n",
    "Use [`torch.cuda.max_memory_allocated()`](https://pytorch.org/docs/stable/cuda.html#torch.cuda.max_memory_allocated) at the end of your script to show the maximum amount of memory in bytes used by all tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fSQdauqLIkf1",
    "outputId": "8bcffc30-637d-461a-8f44-0e444a28caae"
   },
   "outputs": [],
   "source": [
    "x = torch.randn(1000, 1000, 1000, device='cuda:0')\n",
    "print(f\"Peak memory usage by Pytorch tensors: {(torch.cuda.max_memory_allocated() / 1024 / 1024):.2f} Mb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Did this with `log_gpu_memory='all'` argument in `Trainer` construction.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M3RWHxYKBUys"
   },
   "source": [
    "## Gradient based techniques\n",
    "\n",
    "Modern architectures can potentially consume lots and lots of memory even for minibatch of several objects. To handle such cases here we will discuss two simple techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Since checkpointing does not seem to work well with half precision just yet (bugs...), I'll train a FP model first, for reference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 train_baby_resnet.py --full-precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See `v_num=28` in Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M3RWHxYKBUys"
   },
   "source": [
    "### Gradient Checkpointing (3 points)\n",
    "\n",
    "Checkpointing works by trading compute for memory. Rather than storing all intermediate activations of the entire computation graph for computing backward, the checkpointed part does not save intermediate activations, and instead recomputes them in backward pass. It can be applied on any part of a model.\n",
    "\n",
    "See [blogpost](https://medium.com/tensorflow/fitting-larger-networks-into-memory-583e3c758ff9) for kind introduction and different strategies or [article](https://arxiv.org/pdf/1604.06174.pdf) for not kind introduction.\n",
    "\n",
    "**Task**. Use [built-in checkpointing](https://pytorch.org/docs/stable/checkpoint.html), measure the difference in memory/compute \n",
    "\n",
    "**Requirements**. \n",
    "* Try several arrangements for checkpoints\n",
    "* Add the chekpointing as the optional flag into your script\n",
    "* Measure the difference in memory/compute between the different arrangements and baseline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 train_baby_resnet.py --checkpoint-segments 4 --full-precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See `v_num=29` in Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 train_baby_resnet.py --checkpoint-segments 8 --full-precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See `v_num=30` in Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 train_baby_resnet.py --checkpoint-segments 16 --full-precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See `v_num=32` in Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**: Gradient checkpointing sucks - maybe I did something wrong, though this should not be the case. Did exactly shit to reduce memory consumption. Aaaand it doesn't work with half precition anyways..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mjY8LR_GQbTV"
   },
   "source": [
    "### Accumulating gradient for large batches (3 points)\n",
    "We can increase the effective batch size by simply accumulating gradients over multiple forward passes. Note that `loss.backward()` simply adds the computed gradient to `tensor.grad`, so we can call this method multiple times before actually taking an optimizer step. However, this approach might be a little tricky to combine with batch normalization. Do you see why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qbbbO7V0QeGT"
   },
   "outputs": [],
   "source": [
    "effective_batch_size = 1024\n",
    "loader_batch_size = 32\n",
    "batches_per_update = effective_batch_size / loader_batch_size # Updating weights after 8 forward passes\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=loader_batch_size)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "\n",
    "for batch_i, (batch_X, batch_y) in enumerate(dataloader):\n",
    "    l = loss(model(batch_X), batch_y)\n",
    "    l.backward() # Adds gradients\n",
    "  \n",
    "    if (batch_i + 1) % batches_per_update == 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZqxvZWH9Uxtq"
   },
   "source": [
    "**Task**. Explore the trade-off between computation time and memory usage while maintaining the same effective batch size. By effective batch size we mean the number of objects over which the loss is computed before taking a gradient step.\n",
    "\n",
    "**Requirements**\n",
    "\n",
    "* Compare compute between accumulating gradient and gradient checkpointing with similar memory consumptions\n",
    "* Incorporate gradient accumulation into your script with optional argument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**My solution**: \n",
    "Ok, so compare the default (above) with the version below, which will have effective batch size of 1024, by accumulating 8 512-size batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 train_baby_resnet.py --accumulate-batches 4 --full-precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See `v_num=33` in Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**: Yeah fuck comparison with checkpointing, that shit does not work. Otherwise, nifty solution - barely any increase in memory consumption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K3iiJZuhSUR0"
   },
   "source": [
    "## Accuracy vs compute trade-off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fXad1svpSk8f"
   },
   "source": [
    "### Knowledge distillation (6 points)\n",
    "Suppose that we have a large network (*teacher network*) or an ensemble of networks which has a good accuracy. We can like train a much smaller network (*student network*) using the outputs of teacher networks. It turns out that the perfomance could be even better! This approach doesn't help with training speed, but can be quite beneficial when we'd like to reduce the model size for low-memory devices.\n",
    "\n",
    "* https://www.ttic.edu/dl/dark14.pdf\n",
    "* [Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531)\n",
    "* https://medium.com/neural-machines/knowledge-distillation-dc241d7c2322\n",
    "\n",
    "Even the completely different ([article](https://arxiv.org/abs/1711.10433)) architecture can be used in a student model, e.g. you can approximate an autoregressive model (WaveNet) by a non-autoregressive one.\n",
    "\n",
    "**Task:** \n",
    "1. Train good enough (teacher) network, achieve >=35% accuracy on validation set.\n",
    "2. Train small (student) network, achieve 20-25% accuracy, draw a plot \"training and testing errors vs train step index\"\n",
    "3. Distill teacher network with student network, achieve at least +1% improvement in accuracy over student network accuracy.\n",
    "\n",
    "_Please, don't cheat with early-early-early stopping while training of the student network. Make sure, it  converged._\n",
    "\n",
    "**Note**. Logits carry more information than the probabilities after softmax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, I'm doing something wrong here... Whatever, I don't have any more time to waste on this, honestly. Let's move forward and use what we have learned to do better on other problems!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Prepare data (used in both models)\"\"\"\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from data_processing import load_data, prepare_data\n",
    "from model import MyNet, init_fn\n",
    "from training import setup_trainer\n",
    "\n",
    "# Data parameters\n",
    "val_size = 0.1\n",
    "batch_size = 512\n",
    "\n",
    "train_set, val_set, test_set = prepare_data(val_size)\n",
    "\n",
    "# Prepare DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_set, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_set, batch_size=batch_size, num_workers=8, pin_memory=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_set, batch_size=batch_size, num_workers=8, pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Prepare and train the full model\"\"\"\n",
    "\n",
    "warmup_epochs, decay_epochs, initial_channel_size = 5, 55, 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model = MyNet(\n",
    "    len(train_loader),\n",
    "    warmup_epochs,\n",
    "    decay_epochs,\n",
    "    initial_channel_size,\n",
    ").apply(init_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "CUDA_VISIBLE_DEVICES: [0]\n",
      "Using native 16bit precision.\n"
     ]
    }
   ],
   "source": [
    "# Prepare the trainer\n",
    "trainer_full = setup_trainer(\n",
    "    warmup_epochs,\n",
    "    decay_epochs,\n",
    "    1,\n",
    "    'tb_logs',\n",
    "    'full_model',\n",
    "    'checkpoints',\n",
    "    False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "trainer_full.fit(full_model, train_loader, val_loader)\n",
    "trainer_full.save_checkpoint('full_model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_full.test(full_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model = MyNet.load_from_checkpoint('full_model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "CUDA_VISIBLE_DEVICES: [0]\n",
      "Using native 16bit precision.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Prepare and train the mini model\"\"\"\n",
    "\n",
    "from model_mini1 import MiniNet\n",
    "\n",
    "initial_channel_mini = 16\n",
    "mini_decay_epochs = 75\n",
    "\n",
    "mini_model = MiniNet(\n",
    "    len(train_loader),\n",
    "    warmup_epochs,\n",
    "    mini_decay_epochs,\n",
    "    initial_channel_mini,\n",
    ").apply(init_fn)\n",
    "\n",
    "# Prepare the trainer\n",
    "trainer_mini = setup_trainer(\n",
    "    warmup_epochs,\n",
    "    mini_decay_epochs,\n",
    "    1,\n",
    "    'tb_logs',\n",
    "    'mini_model',\n",
    "    'checkpoints',\n",
    "    False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "trainer_mini.fit(mini_model, train_loader, val_loader)\n",
    "trainer_mini.save_checkpoint('mini_model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6a8ed9f5217418d99d2b324a6406757",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "TEST RESULTS\n",
      "{'test_acc': tensor(0.4745), 'test_loss': tensor(2.2301, device='cuda:0')}\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer_mini.test(mini_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(mini_model.state_dict(), 'mini_model.ck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from model_mini1 import MiniNetStudent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_mini = MiniNetStudent(\n",
    "    len(train_loader),\n",
    "    warmup_epochs,\n",
    "    mini_decay_epochs,\n",
    "    initial_channel_mini,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model.freeze() # This may not even be neccesary anymore\n",
    "full_model.eval()\n",
    "full_model.cuda()\n",
    "student_mini.teacher = full_model.forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "CUDA_VISIBLE_DEVICES: [0]\n",
      "Using native 16bit precision.\n"
     ]
    }
   ],
   "source": [
    "trainer_mini_student = setup_trainer(\n",
    "    warmup_epochs,\n",
    "    mini_decay_epochs,\n",
    "    1,\n",
    "    'tb_logs',\n",
    "    'mini_model_student',\n",
    "    'checkpoints',\n",
    "    False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6685ea9ec80d462cbad3ee2a8a86ddd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:25: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_mini_student.fit(student_mini, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TL;DR. Moar techniques on accuracy vs time trade-off (just for your information)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0WOWhqMJSboR"
   },
   "source": [
    "### Tensor type size\n",
    "\n",
    "One of the hyperparameter affecting memory consumption is the precision (e.g. floating point number). The most popular choice is 32 bit however with several hacks* 16 bit arithmetics can save you approximately half of the memory without considerable loss of perfomance. This is called mixed precision training.\n",
    "\n",
    "*https://arxiv.org/pdf/1710.03740.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-xAEF9aJc-43"
   },
   "source": [
    "### Quantization\n",
    "\n",
    "We can actually move further and use even lower precision like 8-bit integers:\n",
    "\n",
    "* https://heartbeat.fritz.ai/8-bit-quantization-and-tensorflow-lite-speeding-up-mobile-inference-with-low-precision-a882dfcafbbd\n",
    "* https://nervanasystems.github.io/distiller/quantization/\n",
    "* https://arxiv.org/abs/1712.05877"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pruning\n",
    "\n",
    "The idea of pruning is to remove unnecessary (in terms of loss) weights. It can be measured in different ways: for example, by the norm of the weights (similar to L1 feature selection), by the magnitude of the activation or via Taylor expansion*.\n",
    "\n",
    "One iteration of pruning consists of two steps:\n",
    "\n",
    "1) Rank weights with some importance measure and remove the least important\n",
    "\n",
    "2) Fine-tune the model\n",
    "\n",
    "This approach is a bit computationally heavy but can lead to drastic (up to 150x) decrease of memory to store the weights. Moreover if you make use of structure in layers you can decrease also compute. For example, the whole convolutional filters can be removed.\n",
    "\n",
    "*https://arxiv.org/pdf/1611.06440.pdf"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "homework_optimization.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
