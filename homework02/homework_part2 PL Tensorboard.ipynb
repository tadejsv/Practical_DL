{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2.2: The Quest For A Better Network\n",
    "\n",
    "In this assignment you will build a monster network to solve Tiny ImageNet image classification.\n",
    "\n",
    "This notebook is intended as a sequel to seminar 3, please give it a try if you haven't done so yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(please read it at least diagonally)\n",
    "\n",
    "* The ultimate quest is to create a network that has as high __accuracy__ as you can push it.\n",
    "* There is a __mini-report__ at the end that you will have to fill in. We recommend reading it first and filling it while you iterate.\n",
    " \n",
    "## Grading\n",
    "* starting at zero points\n",
    "* +20% for describing your iteration path in a report below.\n",
    "* +20% for building a network that gets above 20% accuracy\n",
    "* +10% for beating each of these milestones on __TEST__ dataset:\n",
    "    * 25% (50% points)\n",
    "    * 30% (60% points)\n",
    "    * 32.5% (70% points)\n",
    "    * 35% (80% points)\n",
    "    * 37.5% (90% points)\n",
    "    * 40% (full points)\n",
    "    \n",
    "## Restrictions\n",
    "* Please do NOT use pre-trained networks for this assignment until you reach 40%.\n",
    " * In other words, base milestones must be beaten without pre-trained nets (and such net must be present in the anytask atttachments). After that, you can use whatever you want.\n",
    "* you __can't__ do anything with validation data apart from running the evaluation procedure. Please, split train images on train and validation parts\n",
    "\n",
    "## Tips on what can be done:\n",
    "\n",
    "\n",
    " * __Network size__\n",
    "   * MOAR neurons, \n",
    "   * MOAR layers, ([torch.nn docs](http://pytorch.org/docs/master/nn.html))\n",
    "\n",
    "   * Nonlinearities in the hidden layers\n",
    "     * tanh, relu, leaky relu, etc\n",
    "   * Larger networks may take more epochs to train, so don't discard your net just because it could didn't beat the baseline in 5 epochs.\n",
    "\n",
    "   * Ph'nglui mglw'nafh Cthulhu R'lyeh wgah'nagl fhtagn!\n",
    "\n",
    "\n",
    "### The main rule of prototyping: one change at a time\n",
    "   * By now you probably have several ideas on what to change. By all means, try them out! But there's a catch: __never test several new things at once__.\n",
    "\n",
    "\n",
    "### Optimization\n",
    "   * Training for 100 epochs regardless of anything is probably a bad idea.\n",
    "   * Some networks converge over 5 epochs, others - over 500.\n",
    "   * Way to go: stop when validation score is 10 iterations past maximum\n",
    "   * You should certainly use adaptive optimizers\n",
    "     * rmsprop, nesterov_momentum, adam, adagrad and so on.\n",
    "     * Converge faster and sometimes reach better optima\n",
    "     * It might make sense to tweak learning rate/momentum, other learning parameters, batch size and number of epochs\n",
    "   * __BatchNormalization__ (nn.BatchNorm2d) for the win!\n",
    "     * Sometimes more batch normalization is better.\n",
    "   * __Regularize__ to prevent overfitting\n",
    "     * Add some L2 weight norm to the loss function, PyTorch will do the rest\n",
    "       * Can be done manually or like [this](https://discuss.pytorch.org/t/simple-l2-regularization/139/2).\n",
    "     * Dropout (`nn.Dropout`) - to prevent overfitting\n",
    "       * Don't overdo it. Check if it actually makes your network better\n",
    "   \n",
    "### Convolution architectures\n",
    "   * This task __can__ be solved by a sequence of convolutions and poolings with batch_norm and ReLU seasoning, but you shouldn't necessarily stop there.\n",
    "   * [Inception family](https://hacktilldawn.com/2016/09/25/inception-modules-explained-and-implemented/), [ResNet family](https://towardsdatascience.com/an-overview-of-resnet-and-its-variants-5281e2f56035?gi=9018057983ca), [Densely-connected convolutions (exotic)](https://arxiv.org/abs/1608.06993), [Capsule networks (exotic)](https://arxiv.org/abs/1710.09829)\n",
    "   * Please do try a few simple architectures before you go for resnet-152.\n",
    "   * Warning! Training convolutional networks can take long without GPU. That's okay.\n",
    "     * If you are CPU-only, we still recomment that you try a simple convolutional architecture\n",
    "     * a perfect option is if you can set it up to run at nighttime and check it up at the morning.\n",
    "     * Make reasonable layer size estimates. A 128-neuron first convolution is likely an overkill.\n",
    "     * __To reduce computation__ time by a factor in exchange for some accuracy drop, try using __stride__ parameter. A stride=2 convolution should take roughly 1/4 of the default (stride=1) one.\n",
    " \n",
    "   \n",
    "### Data augmemntation\n",
    "   * getting 5x as large dataset for free is a great \n",
    "     * Zoom-in+slice = move\n",
    "     * Rotate+zoom(to remove black stripes)\n",
    "     * Add Noize (gaussian or bernoulli)\n",
    "   * Simple way to do that (if you have PIL/Image): \n",
    "     * ```from scipy.misc import imrotate,imresize```\n",
    "     * and a few slicing\n",
    "     * Other cool libraries: cv2, skimake, PIL/Pillow\n",
    "   * A more advanced way is to use torchvision transforms:\n",
    "    ```\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    trainset = torchvision.datasets.ImageFolder(root=path_to_tiny_imagenet, train=True, download=True, transform=transform_train)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "    ```\n",
    "   * Or use this tool from Keras (requires theano/tensorflow): [tutorial](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html), [docs](https://keras.io/preprocessing/image/)\n",
    "   * Stay realistic. There's usually no point in flipping dogs upside down as that is not the way you usually see them.\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchsummary\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n",
      "Installing collected packages: torchsummary\n",
      "Successfully installed torchsummary-1.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "from shutil import copyfile\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.metrics import Accuracy\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data downloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tiny_img import download_tinyImg200\n",
    "\n",
    "# DL data, if necessary\n",
    "if False==True:\n",
    "    data_path = '.'\n",
    "    download_tinyImg200(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Test directory, mimicking the ImageFolder structure of train\n",
    "VAL_DIR = 'tiny-imagenet-200/val'\n",
    "TEST_DIR = 'tiny-imagenet-200/Test'\n",
    "\n",
    "val_list = pd.read_csv(VAL_DIR + '/val_annotations.txt', sep='\\t', header=None)\n",
    "\n",
    "if not os.path.isdir(TEST_DIR):\n",
    "    os.mkdir(TEST_DIR)\n",
    "\n",
    "for x in val_list.iterrows():\n",
    "    img = x[1][0]\n",
    "    folder = TEST_DIR + '/' + x[1][1]\n",
    "    \n",
    "    if not os.path.isdir(folder):\n",
    "        os.mkdir(folder)\n",
    "        \n",
    "    copyfile(VAL_DIR + '/images' + '/' + img, folder + '/' + img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MapDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, map_fn):\n",
    "        self.dataset = dataset\n",
    "        self.map = map_fn\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.map(self.dataset[index][0]), self.dataset[index][1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model definition and... everything else pretty much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicModule(nn.Module):\n",
    "    \"\"\"Basic 2 layer 3x3 convnet block\n",
    "    \n",
    "    Contains 2 3*3 convolution layers. If downsampling, the first convolution layer has a stride of 2,\n",
    "    and the input is passed through a 1*1 convolution layer with stride 2 before adding at the end.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_ch, out_ch, downsample=False):\n",
    "        super(BasicModule, self).__init__()\n",
    "\n",
    "        if downsample:\n",
    "            stride = 2\n",
    "            self.downsample = nn.Conv2d(in_ch, out_ch, 1, stride=2)\n",
    "        elif in_ch != out_ch:\n",
    "            stride = 1\n",
    "            self.downsample = nn.Conv2d(in_ch, out_ch, 1, stride=1)\n",
    "        else:\n",
    "            stride = 1\n",
    "            self.downsample = nn.Identity()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1, stride=stride)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(out_ch)\n",
    "        self.bn2 = nn.BatchNorm2d(out_ch)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        out = self.conv1(input)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = out + self.downsample(input)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class MyNet(pl.LightningModule):\n",
    "    \"\"\"Baby ResNet model\n",
    "    \n",
    "    This version includes 4 residual layers and 2 fully connected layers.\n",
    "    \n",
    "    Input: 3*64*64 image\n",
    "    Layer 0: 5*5 convolution with 16 channels and stride 2\n",
    "    Layer 1: 4 residual blocks of 2 3*3 convolutions with 32 channels\n",
    "    Layer 2: 4 residual blocks of 2 3*3 convolutions with 64 channels\n",
    "    Layer 3: 4 residual blocks of 2 3*3 convolutions with 128 channels\n",
    "    \n",
    "    FC1: Layer with 500 neurons (and ReLU activation)\n",
    "    FC2: Layer with 200 neurons\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MyNet, self).__init__()\n",
    "\n",
    "        # Some important variables\n",
    "        self.WARMUP_EPOCHS = 5\n",
    "        self.DECAY_EPOCHS = 55\n",
    "        self.BATCH_SIZE = 512\n",
    "        self.GAMMA = 5\n",
    "        self.VAL_SIZE = int(1e4)\n",
    "\n",
    "        initial_channel = 32\n",
    "\n",
    "        self.metric = Accuracy()\n",
    "\n",
    "        self.layer_0 = nn.Conv2d(3, initial_channel, 5, padding=2, stride=1)\n",
    "        self.layer_1 = self._make_layer(\n",
    "            3, initial_channel, initial_channel * 2\n",
    "        )  # Returns 32 * 32\n",
    "        self.layer_2 = self._make_layer(\n",
    "            6, initial_channel * 2, initial_channel * 4\n",
    "        )  # Returns 16 * 16\n",
    "        self.layer_3 = self._make_layer(\n",
    "            6, initial_channel * 4, initial_channel * 8\n",
    "        )  # Returns 8 * 8\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        self.fc1 = nn.Linear(initial_channel * 8, 200)\n",
    "\n",
    "    def _make_layer(self, n_blocks, in_ch, out_ch, downsample=True):\n",
    "\n",
    "        blocks = [BasicModule(in_ch, out_ch, downsample=downsample)]\n",
    "        for i in range(n_blocks - 1):\n",
    "            blocks.append(BasicModule(out_ch, out_ch))\n",
    "\n",
    "        return nn.Sequential(*blocks)\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        out = self.layer_0(input)\n",
    "        out = self.layer_1(out)\n",
    "        out = self.layer_2(out)\n",
    "        out = self.layer_3(out)\n",
    "        out = self.avg_pool(out)\n",
    "\n",
    "        out = torch.flatten(out, start_dim=1)\n",
    "        out = self.fc1(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def prepare_data(self):\n",
    "        transforms_train = transforms.Compose(\n",
    "            [\n",
    "                transforms.ColorJitter(hue=0.05, saturation=0.05),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomRotation(20, expand=True),\n",
    "                transforms.RandomResizedCrop(64, scale=(0.6, 1.0)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        transforms_val = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Load and split data\n",
    "        full_set = datasets.ImageFolder(root=\"tiny-imagenet-200/train\")\n",
    "        test_set = datasets.ImageFolder(root=\"tiny-imagenet-200/Test\")\n",
    "        print(len(full_set))\n",
    "        train_size, val_size = len(full_set) - self.VAL_SIZE, self.VAL_SIZE\n",
    "        train_set, val_set = torch.utils.data.random_split(\n",
    "            full_set, (train_size, val_size)\n",
    "        )\n",
    "\n",
    "        # The class labels better be the same - else we have a problem\n",
    "        assert (\n",
    "            test_set.class_to_idx == full_set.class_to_idx\n",
    "        ), \"Test and train labels don't match\"\n",
    "\n",
    "        self.train_set = MapDataset(train_set, transforms_train)\n",
    "        self.val_set = MapDataset(val_set, transforms_val)\n",
    "        self.test_set = MapDataset(test_set, transforms_val)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_set,\n",
    "            batch_size=self.BATCH_SIZE,\n",
    "            shuffle=True,\n",
    "            num_workers=8,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_set, batch_size=self.BATCH_SIZE, num_workers=8, pin_memory=True\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_set, batch_size=self.BATCH_SIZE, num_workers=8, pin_memory=True\n",
    "        )\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        steps_per_epoch = len(self.train_dataloader())\n",
    "        total_epochs = self.WARMUP_EPOCHS + self.DECAY_EPOCHS\n",
    "\n",
    "        optimizers = [optim.Adam(self.parameters(), weight_decay=1e-4)]\n",
    "        schedulers = [\n",
    "            {\n",
    "                \"scheduler\": OneCycleLR(\n",
    "                    optimizers[0],\n",
    "                    0.01,\n",
    "                    epochs=total_epochs,\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    pct_start=self.WARMUP_EPOCHS / total_epochs,\n",
    "                ),\n",
    "                \"interval\": \"step\",\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        return optimizers, schedulers\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "\n",
    "        # add logging\n",
    "        tensorboard_logs = {\"train_loss\": loss}\n",
    "        return {\"loss\": loss, \"log\": tensorboard_logs}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        labels_hat = torch.argmax(logits, dim=1)\n",
    "\n",
    "        accuracy = self.metric(y, labels_hat)\n",
    "\n",
    "        return {\"val_loss\": loss, \"val_acc\": accuracy}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "        avg_acc = torch.stack([x[\"val_acc\"] for x in outputs]).mean()\n",
    "\n",
    "        tensorboard_logs = {\"val_loss\": avg_loss, \"val_acc\": avg_acc}\n",
    "        return {\"val_loss\": avg_loss, \"log\": tensorboard_logs}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        labels_hat = torch.argmax(logits, dim=1)\n",
    "\n",
    "        accuracy = self.metric(y, labels_hat)\n",
    "\n",
    "        return {\"test_loss\": loss, \"test_acc\": accuracy}\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"test_loss\"] for x in outputs]).mean()\n",
    "        test_acc = torch.stack([x[\"test_acc\"] for x in outputs]).mean()\n",
    "\n",
    "        tensorboard_logs = {\"test_loss\": avg_loss, \"test_acc\": test_acc}\n",
    "        return {\"test_acc\": test_acc, \"log\": tensorboard_logs}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the net\n",
    "net1 = MyNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, load it from device!\n",
    "# net1 = MyNet.load_from_checkpoint(checkpoint_path=\"winrar.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize all weights (He initialization)\n",
    "def init_fn(m):\n",
    "    if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "net1 = net1.apply(init_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary(net1, (3, 64, 64), device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:1205: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot LR scheduler\n",
    "lr_graph = []\n",
    "dummy_module = nn.Sequential(nn.Linear(1, 1))\n",
    "dummy_opt = optim.SGD(dummy_module.parameters(), lr=0.1)\n",
    "total_epochs = 5 + 55\n",
    "        \n",
    "sch = OneCycleLR(dummy_opt, epochs=total_epochs, steps_per_epoch=1,\n",
    "                  pct_start=5/total_epochs, final_div_factor=1e2, \n",
    "                 div_factor=1e2, max_lr=0.4*1e-2)\n",
    "\n",
    "for i in range(total_epochs):\n",
    "    lr_graph.append(sch.get_lr()[0])\n",
    "    sch.last_epoch += 1\n",
    "\n",
    "plt.plot(lr_graph)\n",
    "plt.yscale('log')\n",
    "plt.title('Learning rate schedule')\n",
    "_ = plt.xlabel('Epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUUUUUUUUUUUUUUUUUUUUUUUUUUUUNNNN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "CUDA_VISIBLE_DEVICES: [0]\n",
      "Using native 16bit precision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-23adec6ad0ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m                     \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnet1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWARMUP_EPOCHS\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnet1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDECAY_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                     weights_summary=None)\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloader, val_dataloaders)\u001b[0m\n\u001b[1;32m    977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle_gpu\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle_gpu_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_tpu\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: no-cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/distrib_parts.py\u001b[0m in \u001b[0;36msingle_gpu_train\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fit'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_gpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;31m# CHOOSE OPTIMIZER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/device_dtype_mixin.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_device\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m         \"\"\"\n\u001b[0;32m--> 462\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    378\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m         \"\"\"\n\u001b[0;32m--> 462\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "logger = pl.loggers.TensorBoardLogger(\"tb_logs\", name=\"my_model\")\n",
    "save_path = os.path.join(os.getcwd(), 'model_checkpoints')\n",
    "early_stop_callback = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=3, mode='min')\n",
    "checkpoint_callback = ModelCheckpoint(filepath=save_path, monitor='val_loss', mode='min')\n",
    "\n",
    "trainer = pl.Trainer(gpus=1, precision=16, amp_level='O2',\n",
    "                    logger=logger, \n",
    "                    early_stop_callback=early_stop_callback,\n",
    "                    checkpoint_callback=checkpoint_callback,\n",
    "                    max_epochs=net1.WARMUP_EPOCHS+net1.DECAY_EPOCHS,\n",
    "                    weights_summary=None)\n",
    "trainer.fit(net1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(net1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When everything is done, please calculate accuracy on `tiny-imagenet-200/val`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = 0.5486805277888844"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final results:\")\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    test_accuracy * 100))\n",
    "\n",
    "if test_accuracy * 100 > 40:\n",
    "    print(\"Achievement unlocked: 110lvl Warlock!\")\n",
    "elif test_accuracy * 100 > 35:\n",
    "    print(\"Achievement unlocked: 80lvl Warlock!\")\n",
    "elif test_accuracy * 100 > 30:\n",
    "    print(\"Achievement unlocked: 70lvl Warlock!\")\n",
    "elif test_accuracy * 100 > 25:\n",
    "    print(\"Achievement unlocked: 60lvl Warlock!\")\n",
    "else:\n",
    "    print(\"We need more magic! Follow instructons below\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "# Report\n",
    "\n",
    "All creative approaches are highly welcome, but at the very least it would be great to mention\n",
    "* the idea;\n",
    "* brief history of tweaks and improvements;\n",
    "* what is the final architecture and why?\n",
    "* what is the training method and, again, why?\n",
    "* Any regularizations and other techniques applied and their effects;\n",
    "\n",
    "\n",
    "There is no need to write strict mathematical proofs (unless you want to).\n",
    " * \"I tried this, this and this, and the second one turned out to be better. And i just didn't like the name of that one\" - OK, but can be better\n",
    " * \"I have analized these and these articles|sources|blog posts, tried that and that to adapt them to my problem and the conclusions are such and such\" - the ideal one\n",
    " * \"I took that code that demo without understanding it, but i'll never confess that and instead i'll make up some pseudoscientific explaination\" - __not_ok__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hi, my name is `Slim Shady`, and here's my story\n",
    "\n",
    "A long time ago in a galaxy far far away, when it was still more than an hour before the deadline, i got an idea:\n",
    "\n",
    "#### I gonna build a neural network, that\n",
    "\n",
    "Was basically a simpler version of the classic [ResNet](https://arxiv.org/abs/1512.03385) network. I could go into details why, but that would just be copying the paper... There wasn't really that much iterating on the architecture part, since I was set on doing a resnet form the beginning. I did mess around with the number/depth of blocks, but this only seemed to impact the speed of training and not the accuracy.\n",
    "\n",
    "Since this is the kind of network when training on GPU really makes a difference, I spent some time learning how to properly do that. After some initial attempts, I realized that training a model with pure pytorch is really messy, especially if you want to do some \"fancy\" stuff like logging, checkpoints, etc. So I started exploring frameworks: I started with `ignite` and moved to `Pytorch Lightning`, which is supposed to be state of the art. Tbh, it's still light years behind keras, but it's getting there.\n",
    "\n",
    "If I move on to non-technical stuff, what really propelled my model into space (15% over full points benchmark, take that) was the use of a warmup LR scheduler. I could write some smart paragraphs on why this is good, but honestly, I like it bc it simply works. The model actually overfits a lot (loss ~1 on train and ~1.9 on val), so there's still lots of room for improvement.\n",
    "\n",
    "#### One day, with no signs of warning,\n",
    "This thing has finally converged and I was shocked at the result! I was most happy about how GPU really sped things up (I am using RTX 2070 Super), and that warmup helped. Just based on my observation I think  it would be good to experiement with warmup a bit more -- the decline would need to be log linear -- and not linear as it is now. I did try to implement this, but beside from faster initial (but not final) convergence, it did not seem to yield any improvement. Also, Pytorch LR schedulers could be improved.\n",
    "\n",
    "I'll be paying a lot more attention to useful LR schedulers moving forward. Perhaps I should also experiement with optimizers a bit, but I've already spent a ton of time here, so I think it's time to move on.\n",
    "\n",
    "#### Finally, after $\\infty$  iterations, 0 mugs of [tea/coffee] (don't really drink either)\n",
    "\n",
    "* accuracy on training: Waiting for Pytorch Lightning to implement this \n",
    "* accuracy on validation: 55%\n",
    "* accuracy on test: 55%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
